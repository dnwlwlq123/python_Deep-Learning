{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ktj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.4.1-cp311-cp311-win_amd64.whl (199.4 MB)\n",
      "   ---------------------------------------- 0.0/199.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/199.4 MB 48.6 MB/s eta 0:00:05\n",
      "    --------------------------------------- 3.7/199.4 MB 47.0 MB/s eta 0:00:05\n",
      "    --------------------------------------- 4.2/199.4 MB 44.4 MB/s eta 0:00:05\n",
      "    --------------------------------------- 4.2/199.4 MB 44.4 MB/s eta 0:00:05\n",
      "    --------------------------------------- 4.2/199.4 MB 44.4 MB/s eta 0:00:05\n",
      "    --------------------------------------- 4.2/199.4 MB 44.4 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 6.0/199.4 MB 19.2 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 8.0/199.4 MB 23.4 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 11.0/199.4 MB 25.1 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 14.7/199.4 MB 54.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 18.0/199.4 MB 59.8 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 21.6/199.4 MB 73.1 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 24.8/199.4 MB 73.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 27.9/199.4 MB 65.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 29.8/199.4 MB 59.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 32.7/199.4 MB 54.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 34.6/199.4 MB 54.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 36.9/199.4 MB 50.1 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 38.6/199.4 MB 46.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 38.8/199.4 MB 43.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 38.8/199.4 MB 43.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 38.8/199.4 MB 43.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 41.1/199.4 MB 28.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 43.9/199.4 MB 28.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 47.7/199.4 MB 32.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 51.0/199.4 MB 72.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 54.9/199.4 MB 73.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 57.8/199.4 MB 72.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 60.2/199.4 MB 65.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 63.7/199.4 MB 65.6 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 66.7/199.4 MB 65.6 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 69.7/199.4 MB 73.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 71.3/199.4 MB 65.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 71.3/199.4 MB 65.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 71.5/199.4 MB 38.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 72.3/199.4 MB 38.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 72.3/199.4 MB 38.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 72.3/199.4 MB 38.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 72.3/199.4 MB 38.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 73.3/199.4 MB 21.8 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 76.3/199.4 MB 21.8 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 78.7/199.4 MB 21.1 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 81.7/199.4 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 81.8/199.4 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 81.8/199.4 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 81.8/199.4 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 81.8/199.4 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 81.8/199.4 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 83.5/199.4 MB 24.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 87.5/199.4 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 90.4/199.4 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 93.0/199.4 MB 65.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 95.4/199.4 MB 65.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 95.9/199.4 MB 46.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 98.6/199.4 MB 46.9 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 101.0/199.4 MB 43.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 103.4/199.4 MB 43.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 106.5/199.4 MB 54.4 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 109.0/199.4 MB 54.4 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 112.7/199.4 MB 65.2 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 116.0/199.4 MB 65.2 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 118.5/199.4 MB 72.6 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 118.5/199.4 MB 72.6 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 118.5/199.4 MB 72.6 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 118.5/199.4 MB 72.6 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 118.5/199.4 MB 72.6 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 118.5/199.4 MB 72.6 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 118.5/199.4 MB 72.6 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 119.5/199.4 MB 23.4 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 122.0/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 125.0/199.4 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 128.5/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 131.8/199.4 MB 65.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 134.9/199.4 MB 73.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------ 137.3/199.4 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 139.7/199.4 MB 59.5 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 142.7/199.4 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 145.7/199.4 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 148.5/199.4 MB 59.5 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 152.0/199.4 MB 65.2 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 154.2/199.4 MB 59.8 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 156.9/199.4 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 160.0/199.4 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 162.8/199.4 MB 59.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 166.6/199.4 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 168.9/199.4 MB 73.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 169.0/199.4 MB 59.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 169.1/199.4 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 169.2/199.4 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 169.3/199.4 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 169.5/199.4 MB 27.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 169.8/199.4 MB 24.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 172.8/199.4 MB 24.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 175.7/199.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 178.9/199.4 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 182.4/199.4 MB 65.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 185.3/199.4 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 188.4/199.4 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 191.8/199.4 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.4/199.4 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  197.5/199.4 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 199.4/199.4 MB 16.8 MB/s eta 0:00:00\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "   ---------------------------------------- 0.0/179.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 179.3/179.3 kB 11.3 MB/s eta 0:00:00\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.3/133.3 kB ? eta 0:00:00\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.7/1.7 MB 54.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 36.0 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 2.0/6.2 MB 42.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.5/6.2 MB 47.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 49.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 39.6 MB/s eta 0:00:00\n",
      "Downloading MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 32.9 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.16.1 fsspec-2024.9.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 sympy-1.13.3 torch-2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-D Tensor\n",
    "scalar = torch.tensor(5.0)\n",
    "number = torch.tensor(1.0)\n",
    "print(scalar)  # tensor(5.)\n",
    "print(number) # tensor(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = torch.tensor([1.0, 2.0, 3.0])\n",
    "tuple_vector = torch.tensor((1, 2, 3))\n",
    "\n",
    "print(vector)  # tensor([1., 2., 3.])\n",
    "print(tuple_vector) # tensor([1, 2, 3,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "matrix2 = torch.tensor([[1, 2, 3], [3, 4, 5]])\n",
    "print(matrix.shape)\n",
    "print(matrix2.shape)\n",
    "# tensor([[1., 2.],\n",
    "#         [3., 4.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix2 = [[1, 2, 3], [3, 4, 5]]\n",
    "lst = [matrix2, matrix2, matrix2, matrix2]\n",
    "\n",
    "tensor_3d = torch.tensor([[[1.0], [2.0]], [[3.0], [4.0]]])\n",
    "tensor_3d_2 = torch.tensor(lst) # len(lst), len(lst[0]), len(lst[0][0]), ....\n",
    "print(tensor_3d_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector.shape)    # torch.Size([3])\n",
    "print(matrix.size())   # torch.Size([2, 2])\n",
    "print(tensor_3d.shape) # torch.Size([2, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector.dtype)    # torch.float32\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "print(int_tensor.dtype)  # torch.int32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tensor_gpu = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
    "print(tensor_gpu.device)  # cuda:0 or cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tensor = torch.tensor([1, 2, 3])\n",
    "tuple_tensor = torch.tensor((4, 5, 6))\n",
    "\n",
    "zeros = torch.zeros((2, 3))\n",
    "ones = torch.ones((2, 3))\n",
    "rand = torch.rand((2, 3))\n",
    "eye = torch.eye(3)  # 3x3 Identity matrix\n",
    "print(zeros)\n",
    "print(ones)\n",
    "print(rand)\n",
    "print(eye)\n",
    "normal = torch.randn((2, 3))  # Normal distribution\n",
    "\n",
    "arange_tensor = torch.arange(start=0, end=10, step=2) # range(0, 10, 2)\n",
    "linspace_tensor = torch.linspace(start=0, end=1, steps=5) # 0 0.25 0.5 0.75 1\n",
    "print(arange_tensor)\n",
    "print(linspace_tensor)\n",
    "float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "import random\n",
    "\n",
    "def nested_list(shape, value = 0):\n",
    "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
    "    \"\"\"\n",
    "    if len(shape) == 1:\n",
    "        l = shape[0]\n",
    "        return [value for _ in range(l)]\n",
    "    else:\n",
    "        l = shape[0]\n",
    "        return [nested_list(shape[1:], value = value) for _ in range(l)]\n",
    "\n",
    "def random_nested_list(shape, sample_from, *args):\n",
    "    if len(shape) == 1:\n",
    "        l = shape[0]\n",
    "        return [sample_from(*args) for _ in range(l)]\n",
    "    else:\n",
    "        l = shape[0]\n",
    "        return [random_nested_list(shape[1:], sample_from, *args) for _ in range(l)]\n",
    "'''\n",
    "def random_nested_list(shape):\n",
    "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
    "    \"\"\"\n",
    "    if len(shape) == 1:\n",
    "        l = shape[0]\n",
    "        return [random.random() for _ in range(l)]\n",
    "    else:\n",
    "        l = shape[0]\n",
    "        return [random_nested_list(shape[1:]) for _ in range(l)]\n",
    "'''\n",
    "def randomn_nested_list(shape):\n",
    "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
    "    \"\"\"\n",
    "    if len(shape) == 1:\n",
    "        l = shape[0]\n",
    "        return [random.gauss(0, 1) for _ in range(l)]\n",
    "    else:\n",
    "        l = shape[0]\n",
    "        return [randomn_nested_list(shape[1:]) for _ in range(l)]\n",
    "\n",
    "def zeros(shape):\n",
    "    return torch.tensor(nested_list(shape, value = 0))\n",
    "\n",
    "def ones(shape):\n",
    "    return torch.tensor(nested_list(shape, value = 1))\n",
    "\n",
    "def rand(shape):\n",
    "    return torch.tensor(random_nested_list(shape, random.random))\n",
    "\n",
    "def randn(shape):\n",
    "    return torch.tensor(random_nested_list(shape, random.gauss, 0, 1))\n",
    "\n",
    "print(nested_list((2, 3, 4), value = 0))\n",
    "print(zeros((2,3,4)) == torch.zeros((2,3,4)))\n",
    "print(ones((2,3,4)))\n",
    "print(rand((2,3,4)))\n",
    "print(randn((2,3,4)))\n",
    "\n",
    "def eyes(n):\n",
    "    lst = [[0 for i in range(n)] for j in range(n)]\n",
    "\n",
    "    lst = []\n",
    "    for i in range(n):\n",
    "        tmp = []\n",
    "        for j in range(n):\n",
    "            tmp.append(0)\n",
    "        lst.append(tmp)\n",
    "\n",
    "    for i in range(n):\n",
    "        lst[i][i] = 1\n",
    "\n",
    "    return torch.tensor(lst)\n",
    "\n",
    "print(eyes(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "add = a + b  # tensor([5., 7., 9.])\n",
    "sub = a - b  # tensor([-3., -3., -3.])\n",
    "\n",
    "mul = a * b  # tensor([ 4., 10., 18.])\n",
    "div = b / a  # tensor([4.0000, 2.5000, 2.0000])\n",
    "\n",
    "exp = a ** 2  # tensor([1., 4., 9.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_a = torch.tensor([[1, 2], [3, 4]])\n",
    "matrix_b = torch.tensor([[5, 6], [7, 8]])\n",
    "'''\n",
    "1 2  5 6\n",
    "3 4  7 8\n",
    "\n",
    "1 3\n",
    "2 4\n",
    "'''\n",
    "'''\n",
    "1*5 + 2*7 = 19   1*6 + 2*8 = 22\n",
    "3*5 + 4*7 = 43   3*6 + 4*8 = 50\n",
    "'''\n",
    "\n",
    "matmul = torch.matmul(matrix_a, matrix_b)\n",
    "\n",
    "# tensor([[19, 22],\n",
    "#         [43, 50]])\n",
    "\n",
    "elem_mul = matrix_a * matrix_b\n",
    "# tensor([[ 5, 12],\n",
    "#         [21, 32]])\n",
    "\n",
    "transposed = torch.transpose(matrix_a, 0, 1)\n",
    "# tensor([[1, 3],\n",
    "#         [2, 4]])\n",
    "\n",
    "print(torch.matmul(torch.tensor([[1],[2],[3]]), torch.tensor([[4,5,6]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])    # Shape: (2, 3,)\n",
    "b = torch.tensor([1, 2, 3])                 # Shape: (3,)\n",
    "b = torch.tensor([[1, 2, 3]])               # Shape: (1, 3,)\n",
    "b = torch.tensor([[1, 2, 3], [1, 2, 3]])    # Shape: (2, 3,)\n",
    "\n",
    "broadcast_add = a + b  # Shape: (2, 3)\n",
    "# tensor([[2, 4, 6],\n",
    "#         [5, 7, 9]])\n",
    "\n",
    "a = torch.tensor([[1], [2], [3]])  # Shape: (3, 1)\n",
    "b = torch.tensor([4, 5, 6])        # Shape: (3,)\n",
    "b = torch.tensor([[4, 5, 6]])        # Shape: (1, 3,)\n",
    "b = torch.tensor([[4, 5, 6], [4, 5, 6], [4, 5, 6]])        # Shape: (3, 3,)\n",
    "a = torch.tensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]])  # Shape: (3, 3)\n",
    "a*b = [[4, 5, 6], [8, 10, 12], [12, 15, 18]]\n",
    "# To make shapes compatible:\n",
    "# a: (3, 1) -> (3, 3)\n",
    "# b: (3,)   -> (1, 3) -> (3, 3)\n",
    "\n",
    "broadcast_mul = a * b  # Shape: (3, 3)\n",
    "# tensor([[ 4,  5,  6],\n",
    "#         [ 8, 10, 12],\n",
    "#         [12, 15, 18]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([-1.0, -2.0, 3.0])\n",
    "\n",
    "abs_a = torch.abs(a)          # tensor([1., 2., 3.])\n",
    "# sqrt_a = torch.sqrt(a)\n",
    "sqrt_a = torch.sqrt(torch.abs(a))  # tensor([1., 1.4142, 1.7321])\n",
    "exp_a = torch.exp(a)          # tensor([0.3679, 0.1353, 20.0855])\n",
    "log_a = torch.log(torch.abs(a))    # tensor([0.0000, 0.6931, 1.0986])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "max_ab = torch.max(a, b)  # tensor([4., 5., 6.])\n",
    "min_ab = torch.min(a, b)  # tensor([1., 2., 3.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[1, 2, 3], [3, 4, 5]]) # (2,3)\n",
    "\n",
    "sum_all = torch.sum(tensor)          # tensor(10)\n",
    "sum_dim0 = torch.sum(tensor, dim=0)  # tensor([4, 6, 8]) (3,)\n",
    "sum_dim1 = torch.sum(tensor, dim=1)  # tensor([6, 12]) (2,)\n",
    "\n",
    "mean_all = torch.mean(tensor.float(), dim = 1)  # tensor(2.5000)\n",
    "print(mean_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([2, 2, 2])\n",
    "\n",
    "greater = a > b  # tensor([False, False, True])\n",
    "equal = a == b   # tensor([False, True, False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의로 데이터를 한번 만들어 보자.\n",
    "# True parameters\n",
    "true_w = 2.0\n",
    "true_b = 1.0\n",
    "\n",
    "# Generate data\n",
    "X = torch.randn(100, 1) * 10  # 100 samples, single feature\n",
    "y = true_w * X + true_b + torch.randn(100, 1) * 0.2  # Add noise\n",
    "\n",
    "# y[0]= true_w * X[0] + true_b + torch.randn(1, 1) * 2\n",
    "# y[1]= true_w * X[1] + true_b + torch.randn(1, 1) * 2\n",
    "# ...\n",
    "# y[99]= true_w * X[99] + true_b + torch.randn(1, 1) * 2\n",
    "\n",
    "# requires_grad = True로 해야 학습이 가능\n",
    "w = torch.randn(1, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.005\n",
    "epochs = 5000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
    "    # Forward pass: compute predicted y\n",
    "    # 100, 1 / 1 -> 100, 1 / 1, 1 -> 100, 1 / 100, 1\n",
    "    y_pred = X * w + b # y_pred: 100, 1\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = torch.mean((y_pred - y) ** 2) # 100, 1\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # Zero gradients after updating\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = w1*x1 + w2 * x2 + w3 * x3 + b\n",
    "import torch\n",
    "\n",
    "def generate_artificial_data(true_w, true_b, n_data, x_amplitude = 10, noise_amplitude = 0.2):\n",
    "  X = torch.randn(n_data, 3) * x_amplitude\n",
    "  y = (true_w[0] * X[:, 0] + true_w[1] * X[:, 1] + true_w[2] * X[:, 2] + true_b + torch.randn(n_data) * noise_amplitude)\n",
    "  return X, y.view(-1, 1)\n",
    "def linear_regression(data, learning_rate, epochs, quiet = False):\n",
    "  X, y = data\n",
    "  w = torch.randn(3, 1, requires_grad = True)\n",
    "  b = torch.randn(1, requires_grad = True)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    y_pred = X @ w + b\n",
    "\n",
    "    loss = torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "    if epoch % 100 == 0 and not quiet:\n",
    "        print(f\"Epoch {epoch} : w = {w.view(-1).tolist()}, b = {b.item():.4f}, loss = {loss.item():.4f}\")\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      w -= learning_rate * w.grad\n",
    "      b -= learning_rate * b.grad\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "  return w, b\n",
    "def evaluate_model(true_w, true_b, w, b):\n",
    "\n",
    "    return (torch.mean((true_w - w.view(-1)) ** 2 / true_w ** 2) + (true_b - b) ** 2 / true_b ** 2).item()\n",
    "\n",
    "def find_learning_rate():\n",
    "\n",
    "    true_w = torch.tensor([2.0, -1.0, 3.0]) \n",
    "    true_b = 1.0\n",
    "    data = generate_artificial_data(true_w, true_b, 1000)  \n",
    "    for learning_rate in [0.0001 * (i + 1) for i in range(10)]:\n",
    "        w, b = linear_regression(data, learning_rate, 1000, quiet=True)\n",
    "        score = evaluate_model(true_w, true_b, w, b)\n",
    "        print(f'Score: {score:.6f}, Learning Rate: {learning_rate}')\n",
    "\n",
    "data = generate_artificial_data(torch.tensor([2.0, -1.0, 3.0]), 1.0, 1000)\n",
    "linear_regression(data, 0.001, 1000)\n",
    "\n",
    "\n",
    "find_learning_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
