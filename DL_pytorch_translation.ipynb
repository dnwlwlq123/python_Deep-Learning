{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA 설정\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# NLTK 다운로드\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 데이터 샘플\n",
    "data = [\n",
    "    (\"hello\", \"안녕\"),\n",
    "    (\"python\", \"파이썬\"),\n",
    "    (\"lol\", \"리그오브레전드\"),\n",
    "    (\"java\", \"자바\"),\n",
    "    (\"apple\", \"사과\"),\n",
    "    (\"dl\", \"딥러닝\"),\n",
    "    (\"ml\", \"머신러닝\"),\n",
    "    (\"computer\", \"컴퓨터\"),\n",
    "    (\"programming\", \"프로그래밍\"),\n",
    "    (\"language\", \"언어\"),\n",
    "    (\"artificial\", \"인공적인\"),\n",
    "    (\"intelligence\", \"지능\"),\n",
    "    (\"neural\", \"신경\"),\n",
    "    (\"network\", \"네트워크\"),\n",
    "    (\"data\", \"데이터\"),\n",
    "    (\"science\", \"과학\"),\n",
    "    (\"algorithm\", \"알고리즘\"),\n",
    "    (\"model\", \"모델\"),\n",
    "    (\"training\", \"훈련\"),\n",
    "    (\"learning\", \"학습\"),\n",
    "    (\"hello\", \"안녕\"),\n",
    "    (\"python\", \"파이썬\"),\n",
    "    (\"lol\", \"리그오브레전드\"),\n",
    "    (\"java\", \"자바\"),\n",
    "    (\"apple\", \"사과\"),\n",
    "    (\"dl\", \"딥러닝\"),\n",
    "    (\"ml\", \"머신러닝\"),\n",
    "    (\"computer\", \"컴퓨터\"),\n",
    "    (\"programming\", \"프로그래밍\"),\n",
    "    (\"language\", \"언어\"),\n",
    "    (\"artificial\", \"인공적인\"),\n",
    "    (\"intelligence\", \"지능\"),\n",
    "    (\"neural\", \"신경\"),\n",
    "    (\"network\", \"네트워크\"),\n",
    "    (\"data\", \"데이터\"),\n",
    "    (\"science\", \"과학\"),\n",
    "    (\"algorithm\", \"알고리즘\"),\n",
    "    (\"model\", \"모델\"),\n",
    "    (\"training\", \"훈련\"),\n",
    "    (\"hello\", \"안녕\"),\n",
    "    (\"python\", \"파이썬\"),\n",
    "    (\"lol\", \"리그오브레전드\"),\n",
    "    (\"java\", \"자바\"),\n",
    "    (\"apple\", \"사과\"),\n",
    "    (\"dl\", \"딥러닝\"),\n",
    "    (\"ml\", \"머신러닝\"),\n",
    "    (\"computer\", \"컴퓨터\"),\n",
    "    (\"programming\", \"프로그래밍\"),\n",
    "    (\"language\", \"언어\"),\n",
    "    (\"artificial\", \"인공적인\"),\n",
    "    (\"intelligence\", \"지능\"),\n",
    "    (\"neural\", \"신경\"),\n",
    "    (\"network\", \"네트워크\"),\n",
    "    (\"data\", \"데이터\"),\n",
    "    (\"science\", \"과학\"),\n",
    "    (\"algorithm\", \"알고리즘\"),\n",
    "    (\"model\", \"모델\"),\n",
    "    (\"training\", \"훈련\"),\n",
    "    (\"hello\", \"안녕\"),\n",
    "    (\"python\", \"파이썬\"),\n",
    "    (\"lol\", \"리그오브레전드\"),\n",
    "    (\"java\", \"자바\"),\n",
    "    (\"apple\", \"사과\"),\n",
    "    (\"dl\", \"딥러닝\"),\n",
    "    (\"ml\", \"머신러닝\"),\n",
    "    (\"computer\", \"컴퓨터\"),\n",
    "    (\"programming\", \"프로그래밍\"),\n",
    "    (\"language\", \"언어\"),\n",
    "    (\"artificial\", \"인공적인\"),\n",
    "    (\"intelligence\", \"지능\"),\n",
    "    (\"neural\", \"신경\"),\n",
    "    (\"network\", \"네트워크\"),\n",
    "    (\"data\", \"데이터\"),\n",
    "    (\"science\", \"과학\"),\n",
    "    (\"algorithm\", \"알고리즘\"),\n",
    "    (\"model\", \"모델\"),\n",
    "    (\"training\", \"훈련\"),\n",
    "    (\"hello\", \"안녕\"),\n",
    "    (\"python\", \"파이썬\"),\n",
    "    (\"lol\", \"리그오브레전드\"),\n",
    "    (\"java\", \"자바\"),\n",
    "    (\"apple\", \"사과\"),\n",
    "    (\"dl\", \"딥러닝\"),\n",
    "    (\"ml\", \"머신러닝\"),\n",
    "    (\"computer\", \"컴퓨터\"),\n",
    "    (\"programming\", \"프로그래밍\"),\n",
    "    (\"language\", \"언어\"),\n",
    "    (\"artificial\", \"인공적인\"),\n",
    "    (\"intelligence\", \"지능\"),\n",
    "    (\"neural\", \"신경\"),\n",
    "    (\"network\", \"네트워크\"),\n",
    "    (\"data\", \"데이터\"),\n",
    "    (\"science\", \"과학\"),\n",
    "    (\"algorithm\", \"알고리즘\"),\n",
    "    (\"model\", \"모델\"),\n",
    "    (\"training\", \"훈련\"),\n",
    "    (\"hello\", \"안녕\"),\n",
    "    (\"python\", \"파이썬\"),\n",
    "    (\"lol\", \"리그오브레전드\"),\n",
    "    (\"java\", \"자바\"),\n",
    "    (\"apple\", \"사과\"),\n",
    "    (\"dl\", \"딥러닝\"),\n",
    "    (\"ml\", \"머신러닝\"),\n",
    "    (\"computer\", \"컴퓨터\"),\n",
    "    (\"programming\", \"프로그래밍\"),\n",
    "    (\"language\", \"언어\"),\n",
    "    (\"artificial\", \"인공적인\"),\n",
    "    (\"intelligence\", \"지능\"),\n",
    "    (\"neural\", \"신경\"),\n",
    "    (\"network\", \"네트워크\"),\n",
    "    (\"data\", \"데이터\"),\n",
    "    (\"science\", \"과학\"),\n",
    "    (\"algorithm\", \"알고리즘\"),\n",
    "    (\"model\", \"모델\"),\n",
    "    (\"hello\", \"안녕\"),\n",
    "    (\"python\", \"파이썬\"),\n",
    "    (\"lol\", \"리그오브레전드\"),\n",
    "    (\"java\", \"자바\"),\n",
    "    (\"apple\", \"사과\"),\n",
    "    (\"dl\", \"딥러닝\"),\n",
    "    (\"ml\", \"머신러닝\"),\n",
    "    (\"computer\", \"컴퓨터\"),\n",
    "    (\"programming\", \"프로그래밍\"),\n",
    "    (\"language\", \"언어\"),\n",
    "    (\"artificial\", \"인공적인\"),\n",
    "    (\"intelligence\", \"지능\"),\n",
    "    (\"neural\", \"신경\"),\n",
    "    (\"network\", \"네트워크\"),\n",
    "    (\"data\", \"데이터\"),\n",
    "    (\"science\", \"과학\"),\n",
    "    (\"algorithm\", \"알고리즘\"),\n",
    "    (\"model\", \"모델\"),\n",
    "    (\"hello\", \"안녕\"),\n",
    "    (\"python\", \"파이썬\"),\n",
    "    (\"lol\", \"리그오브레전드\"),\n",
    "    (\"java\", \"자바\"),\n",
    "    (\"apple\", \"사과\"),\n",
    "    (\"dl\", \"딥러닝\"),\n",
    "    (\"ml\", \"머신러닝\"),\n",
    "    (\"computer\", \"컴퓨터\"),\n",
    "    (\"programming\", \"프로그래밍\"),\n",
    "    (\"language\", \"언어\"),\n",
    "    (\"artificial\", \"인공적인\"),\n",
    "    (\"intelligence\", \"지능\"),\n",
    "    (\"neural\", \"신경\"),\n",
    "    (\"network\", \"네트워크\"),\n",
    "    (\"data\", \"데이터\"),\n",
    "    (\"science\", \"과학\"),\n",
    "    (\"algorithm\", \"알고리즘\"),\n",
    "    (\"model\", \"모델\"),\n",
    "    (\"hello\", \"안녕\"),\n",
    "    (\"python\", \"파이썬\"),\n",
    "    (\"lol\", \"리그오브레전드\"),\n",
    "    (\"java\", \"자바\"),\n",
    "    (\"apple\", \"사과\"),\n",
    "    (\"dl\", \"딥러닝\"),\n",
    "    (\"ml\", \"머신러닝\"),\n",
    "    (\"computer\", \"컴퓨터\"),\n",
    "    (\"programming\", \"프로그래밍\"),\n",
    "    (\"language\", \"언어\"),\n",
    "    (\"artificial\", \"인공적인\"),\n",
    "    (\"intelligence\", \"지능\"),\n",
    "    (\"neural\", \"신경\"),\n",
    "    (\"network\", \"네트워크\"),\n",
    "    (\"data\", \"데이터\"),\n",
    "    (\"science\", \"과학\"),\n",
    "    (\"algorithm\", \"알고리즘\"),\n",
    "    (\"model\", \"모델\"),\n",
    "]\n",
    "\n",
    "\n",
    "class TranslationData(Dataset):\n",
    "    def __init__(self, data, max_length=100):\n",
    "        self.eng_sentence = [word[0] for word in data]\n",
    "        self.kor_sentence = [word[1] for word in data]\n",
    "        self.eng_vocab = self.build_vocab(self.eng_sentence)\n",
    "        self.kor_vocab = self.build_vocab(self.kor_sentence)\n",
    "\n",
    "        self.sos_idx = self.kor_vocab['<sos>']\n",
    "        self.eos_idx = self.kor_vocab['<eos>']\n",
    "        self.padding_idx = self.kor_vocab['<pad>']\n",
    "\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(\"영어 문장:\", self.eng_sentence)\n",
    "        print(\"한국어 문장:\", self.kor_sentence)\n",
    "        print(\"영어 어휘:\", self.eng_vocab)\n",
    "        print(\"한국어 어휘:\", self.kor_vocab)\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n",
    "        for sentence in sentences:\n",
    "            for word in word_tokenize(sentence):\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "        return vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.kor_sentence)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng = self.eng_sentence[idx]\n",
    "        kor = self.kor_sentence[idx]\n",
    "\n",
    "\n",
    "        eng_indices = [self.eng_vocab[word] for word in word_tokenize(eng) if word in self.eng_vocab]\n",
    "\n",
    "        kor_indices = [self.sos_idx] + [self.kor_vocab[word] for word in word_tokenize(kor) if word in self.kor_vocab] + [self.eos_idx]\n",
    "        # 패딩 추가\n",
    "        if len(eng_indices) < self.max_length:\n",
    "          eng_indices += [self.padding_idx] * (self.max_length - len(eng_indices))\n",
    "        else:\n",
    "          eng_indices = eng_indices[:self.max_length]  # max_length 초과시 잘라냄\n",
    "\n",
    "        if len(kor_indices) < self.max_length:\n",
    "          kor_indices += [self.padding_idx] * (self.max_length - len(kor_indices))\n",
    "        else:\n",
    "          kor_indices = kor_indices[:self.max_length]\n",
    "\n",
    "        return torch.tensor(eng_indices), torch.tensor(kor_indices)\n",
    "\n",
    "dataset = TranslationData(data)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        batch_size, seq_length = input_seq.size()\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size).to(input_seq.device)\n",
    "\n",
    "        for char_idx in range(seq_length):\n",
    "            x_t = self.embedding(input_seq[:, char_idx])\n",
    "            hidden = self.activation(self.linear(x_t) + hidden)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, self.output_size).to(input.device)\n",
    "\n",
    "        hidden = hidden[-1]\n",
    "\n",
    "        for char_idx in range(seq_len):\n",
    "            if char_idx == 0:\n",
    "                previous_y = torch.zeros(batch_size, self.hidden_size).to(input.device)\n",
    "            else:\n",
    "                y_prev = input[:, char_idx - 1]\n",
    "                previous_y = self.embedding(y_prev).view(batch_size, -1)\n",
    "\n",
    "            hidden = self.activation(self.linear1(previous_y) + hidden)\n",
    "            output = self.linear2(hidden)\n",
    "\n",
    "            outputs[:, char_idx, :] = output\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output_size = decoder.output_size\n",
    "\n",
    "    def forward(self, input_seq, decoder_target, teacher_forcing_ratio=0.1):\n",
    "        encoder_hidden = self.encoder(input_seq)\n",
    "        input_seq = input_seq.to(device)\n",
    "        decoder_input = decoder_target[0].unsqueeze(0).to(device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(decoder_target.size(0)):\n",
    "            if t == 0:\n",
    "                decoder_output, encoder_hidden = self.decoder(decoder_input, encoder_hidden)\n",
    "            else:\n",
    "                decoder_output, encoder_hidden = self.decoder(decoder_input, encoder_hidden)\n",
    "\n",
    "            decoder_output = decoder_output.view(1, -1, self.output_size)\n",
    "            outputs.append(decoder_output)\n",
    "\n",
    "            if t < decoder_target.size(0) - 1:\n",
    "                top1 = decoder_output.argmax(2)\n",
    "                decoder_input = decoder_target[t + 1].unsqueeze(0).to(device) if random.random() < teacher_forcing_ratio else top1.to(device)\n",
    "            else:\n",
    "                decoder_input = decoder_output\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "        return outputs\n",
    "\n",
    "def translate(model, word, dataset, device):\n",
    "    if word not in dataset.eng_vocab:\n",
    "        return \"데이터에 해당 단어가 없습니다.\"\n",
    "\n",
    "    eng_indices = [dataset.eng_vocab[word]]\n",
    "    eng_indices += [dataset.padding_idx] * (dataset.max_length - len(eng_indices))\n",
    "    test_input = torch.tensor(eng_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_target = torch.zeros((1, dataset.max_length), dtype=torch.long).to(device)\n",
    "        test_target[0][0] = dataset.kor_vocab['<sos>']\n",
    "        output = model(test_input, test_target)\n",
    "\n",
    "        predicted = torch.argmax(output, dim=2)\n",
    "        for pred in predicted[0]:\n",
    "            if pred.item() not in {dataset.padding_idx, dataset.sos_idx, dataset.eos_idx}:\n",
    "                translated_word = list(dataset.kor_vocab.keys())[list(dataset.kor_vocab.values()).index(pred.item())]\n",
    "                return translated_word\n",
    "\n",
    "        # return \" \".join(translated_sentence)\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs, device):\n",
    "    model.train()  \n",
    "    loss_values = []  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0  \n",
    "        for eng, kor in dataloader:\n",
    "            eng, kor = eng.to(device), kor.to(device)\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            output = model(eng, kor[:, :-1])  \n",
    "            output = output.view(-1, output.size(-1)) \n",
    "\n",
    "            # 손실 계산\n",
    "            loss = criterion(output, kor[:, 1:].contiguous().view(-1))\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                print(\"손실이 NaN입니다.\")\n",
    "                continue\n",
    "\n",
    "            loss.backward()  # 역전파\n",
    "            optimizer.step()  # 가중치 업데이트\n",
    "            epoch_loss += loss.item()  # 에포크 손실 누적\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)  # 평균 손실 계산\n",
    "        loss_values.append(avg_loss)  # 리스트에 평균 손실 추가\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return loss_values  # 손실 값을 반환\n",
    "\n",
    "# 손실 값 리스트를 받는 코드\n",
    "loss_values = train_model(model, dataloader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# 손실 값 그래프 그리기\n",
    "plt.plot(loss_values)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show() \n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for eng, kor in dataloader:\n",
    "            eng, kor = eng.to(device), kor.to(device)\n",
    "            output = model(eng, kor[:, :-1])\n",
    "            predicted = torch.argmax(output, dim=2)\n",
    "\n",
    "            for i in range(predicted.size(0)):\n",
    "                for j in range(predicted.size(1)):\n",
    "                    if kor[i, j + 1] != dataset.padding_idx:\n",
    "                        total += 1\n",
    "                        correct += (predicted[i, j] == kor[i, j + 1]).item()\n",
    "\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    max_length = 10\n",
    "    hidden_size = 100\n",
    "    batch_size = 100\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "    dataset = TranslationData(data, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(device)\n",
    "\n",
    "\n",
    "    eng_vocab_size = len(dataset.eng_vocab)\n",
    "    kor_vocab_size = len(dataset.kor_vocab)\n",
    "    # print(\"Encoder input size:\", eng_vocab_size)\n",
    "    # print(\"Decoder input size:\", kor_vocab_size)\n",
    "    # print(\"Decoder output size:\", kor_vocab_size)\n",
    "    # print(\"Input sequence device:\", input_seq.device)\n",
    "    # print(\"Target sequence device:\", input.device)\n",
    "\n",
    "\n",
    "    try:\n",
    "        encoder = Encoder(input_size=eng_vocab_size, hidden_size=hidden_size).to(device)\n",
    "        decoder = Decoder(input_size=kor_vocab_size, hidden_size=hidden_size, output_size=kor_vocab_size).to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"모델 초기화 중 오류 발생: {e}\")\n",
    "\n",
    "    model = Seq2Seq(encoder, decoder).to(device)\n",
    "    for input_seq, target_seq in dataloader:\n",
    "      input_seq = input_seq.to(device)\n",
    "      target_seq = target_seq.to(device)\n",
    "      output = model(input_seq, target_seq)\n",
    "      # print(\"모델 출력:\", output)\n",
    "      print(\"Seq2Seq output shape:\", output.shape)  # Seq2Seq 출력 모양 출력\n",
    "      break\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.padding_idx)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    train_model(model, dataloader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "\n",
    "    acc = evaluate_model(model, dataloader, device)\n",
    "    print(f\"Training Accuracy: {acc * 100:.2f}%\\n\")\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        word = input(\"영단어 입력: \")\n",
    "        if word.lower() == 'exit':\n",
    "            break\n",
    "        result = translate(model, word, dataset, device)\n",
    "        print(\"번역 결과:\", result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
