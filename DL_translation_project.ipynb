{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TranslationData(Dataset):\n",
    "    def __init__(self, data, max_length=100):\n",
    "        self.eng_sentence = [word[0] for word in data]\n",
    "        self.kor_sentence = [word[1] for word in data]\n",
    "        self.eng_vocab = self.build_vocab(self.eng_sentence)\n",
    "        self.kor_vocab = self.build_vocab(self.kor_sentence)\n",
    "\n",
    "        self.sos_idx = self.kor_vocab['<sos>']\n",
    "        self.eos_idx = self.kor_vocab['<eos>']\n",
    "        self.padding_idx = self.kor_vocab['<pad>']\n",
    "\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(\"영어 문장:\", self.eng_sentence)\n",
    "        print(\"한국어 문장:\", self.kor_sentence)\n",
    "        print(\"영어 어휘:\", self.eng_vocab)\n",
    "        print(\"한국어 어휘:\", self.kor_vocab)\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n",
    "        for sentence in sentences:\n",
    "            for word in word_tokenize(sentence):\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "        return vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.kor_sentence)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng = self.eng_sentence[idx]\n",
    "        kor = self.kor_sentence[idx]\n",
    "\n",
    "\n",
    "        eng_indices = [self.eng_vocab[word] for word in word_tokenize(eng) if word in self.eng_vocab]\n",
    "\n",
    "        kor_indices = [self.sos_idx] + [self.kor_vocab[word] for word in word_tokenize(kor) if word in self.kor_vocab] + [self.eos_idx]\n",
    "\n",
    "        if len(eng_indices) < self.max_length:\n",
    "          eng_indices += [self.padding_idx] * (self.max_length - len(eng_indices))\n",
    "        else:\n",
    "          eng_indices = eng_indices[:self.max_length]\n",
    "\n",
    "        if len(kor_indices) < self.max_length:\n",
    "          kor_indices += [self.padding_idx] * (self.max_length - len(kor_indices))\n",
    "        else:\n",
    "          kor_indices = kor_indices[:self.max_length]\n",
    "\n",
    "        return torch.tensor(eng_indices), torch.tensor(kor_indices)\n",
    "\n",
    "dataset = TranslationData(data)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        output, hidden = self.gru(embedded.transpose(0, 1))\n",
    "\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.out(output.squeeze(0))\n",
    "        return output, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "    def forward(self, input_seq, target_seq, teacher_forcing_ratio=0.5):\n",
    "        batch_size = input_seq.size(0)\n",
    "        target_length = target_seq.size(1)\n",
    "        target_vocab_size = self.decoder.output_size\n",
    "        outputs = torch.zeros(batch_size, target_length, target_vocab_size).to(device)\n",
    "\n",
    "        encoder_output, hidden = self.encoder(input_seq)\n",
    "\n",
    "        if hidden.size(0) != 1:\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "\n",
    "        decoder_input = target_seq[:, 0]\n",
    "\n",
    "        for t in range(1, target_length):\n",
    "            output, hidden = self.decoder(decoder_input, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            decoder_input = target_seq[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "def translate(model, word, dataset, device):\n",
    "    if word not in dataset.eng_vocab:\n",
    "        return \"데이터에 해당 단어가 없습니다.\"\n",
    "\n",
    "    eng_indices = [dataset.eng_vocab[word]]\n",
    "    eng_indices += [dataset.padding_idx] * (dataset.max_length - len(eng_indices))\n",
    "    test_input = torch.tensor(eng_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_target = torch.zeros((1, dataset.max_length), dtype=torch.long).to(device)\n",
    "        test_target[0][0] = dataset.kor_vocab['<sos>']\n",
    "        output = model(test_input, test_target)\n",
    "\n",
    "        predicted = torch.argmax(output, dim=2)\n",
    "        for pred in predicted[0]:\n",
    "            if pred.item() not in {dataset.padding_idx, dataset.sos_idx, dataset.eos_idx}:\n",
    "                translated_word = list(dataset.kor_vocab.keys())[list(dataset.kor_vocab.values()).index(pred.item())]\n",
    "                return translated_word\n",
    "\n",
    "        # return \" \".join(translated_sentence)\n",
    "\n",
    "\n",
    "def prepare_data(dataset, batch_size, train_ratio=0.8):\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    train_loss_values = []\n",
    "    val_loss_values = []\n",
    "    train_accuracy_values = []\n",
    "    val_accuracy_values = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_acc = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "        train_loss_values.append(train_loss)\n",
    "        train_accuracy_values.append(train_acc)\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, val_acc = evaluate_epoch(model, val_dataloader, criterion, device)\n",
    "        val_loss_values.append(val_loss)\n",
    "        val_accuracy_values.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, 훈련 손실{train_loss:.4f}, 훈련 정확도: {train_acc:.2f}%, 검증 손실: {val_loss:.4f}, 검증 정확도: {val_acc:.2f}%')\n",
    "\n",
    "    return train_loss_values, val_loss_values, train_accuracy_values, val_accuracy_values\n",
    "\n",
    "def evaluate_epoch(model, dataloader, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for eng, kor in dataloader:\n",
    "            eng, kor = eng.to(device), kor.to(device)\n",
    "            \n",
    "            output = model(eng, kor[:, :-1])\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            kor_target = kor[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, kor_target)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            output_probs = F.softmax(output, dim=-1)\n",
    "            predicted_indices = output_probs.argmax(dim=-1).view(kor.size(0), -1)\n",
    "          \n",
    "\n",
    "            # print(f\"모델 출력: {output.cpu().numpy()}\")\n",
    "            # print(f\"타겟: {kor_target.cpu().numpy()}\")\n",
    "            \n",
    "            # for i in range(eng.size(0)):\n",
    "            #     eng_sentence = [\n",
    "            #         list(dataset.eng_vocab.keys())[list(dataset.eng_vocab.values()).index(idx.item())]\n",
    "            #         for idx in eng[i] if idx.item() != dataset.padding_idx\n",
    "            #     ]\n",
    "            #     target_sentence = [\n",
    "            #         list(dataset.kor_vocab.keys())[list(dataset.kor_vocab.values()).index(idx.item())]\n",
    "            #         for idx in kor[i][1:] if idx.item() not in {dataset.padding_idx, dataset.sos_idx, dataset.eos_idx}\n",
    "            #     ]\n",
    "            #     predicted_sentence = [\n",
    "            #         list(dataset.kor_vocab.keys())[list(dataset.kor_vocab.values()).index(idx.item())]\n",
    "            #         for idx in predicted_indices[i] if idx.item() not in {dataset.padding_idx, dataset.sos_idx, dataset.eos_idx}\n",
    "            #     ]\n",
    "\n",
    "            #     print(f\"예측된 인덱스: {predicted_indices[i].cpu().numpy()}\") \n",
    "\n",
    "\n",
    "            #     if predicted_sentence: \n",
    "            #         predicted_output = ' '.join(predicted_sentence)\n",
    "            #     else:  # 예측된 문장이 비어있는 경우\n",
    "            #         predicted_output = \"비어있음\"\n",
    "\n",
    "\n",
    "            #     print(f\"영어 입력: {' '.join(eng_sentence)}\")\n",
    "            #     print(f\"정답 한국어: {' '.join(target_sentence)}\")\n",
    "            #     print(f\"예측된 한국어: {' '.join(predicted_output)}\")\n",
    "            #     print(\"-\" * 50)\n",
    "                \n",
    "            correct += (predicted_indices.view(-1) == kor_target.view(-1)).sum().item() \n",
    "            total += kor_target.numel()\n",
    "    avg_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
    "    accuracy = (correct / total * 100) if total > 0 else 0.0\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def plot_loss_and_accuracy(train_loss, val_loss, train_accuracy, val_accuracy):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracy, label='Training Accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    max_length = 10\n",
    "    hidden_size = 100\n",
    "    batch_size = 128\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    dataset = TranslationData(data, max_length)\n",
    "    train_dataloader, val_dataloader = prepare_data(dataset, batch_size)\n",
    "    \n",
    "    eng_vocab_size = len(dataset.eng_vocab)\n",
    "    kor_vocab_size = len(dataset.kor_vocab)\n",
    "    \n",
    "    encoder = Encoder(input_size=eng_vocab_size, hidden_size=hidden_size).to(device)\n",
    "    decoder = Decoder(input_size=kor_vocab_size, hidden_size=hidden_size, output_size=kor_vocab_size).to(device)\n",
    "    model = Seq2Seq(encoder, decoder).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_values, val_loss_values, train_accuracy_values, val_accuracy_values = train_model(\n",
    "        model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs, device\n",
    "    )\n",
    "    \n",
    "    plot_loss_and_accuracy(train_loss_values, val_loss_values, train_accuracy_values, val_accuracy_values)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
